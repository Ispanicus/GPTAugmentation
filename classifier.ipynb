{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b93c087-848a-46a2-8710-8bd2f88ea8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a2ac010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639f4dc833114ee192d31d59df9f57f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0 acc 0.776\n",
      "Time taken: 4.357951700000285\n",
      "Fitted vocab size: 541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba271f7b0a4d46e3b134b2048a145a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.05 acc 0.786\n",
      "Time taken: 8.680736000000252\n",
      "Fitted vocab size: 541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15b8dcb14334b978c88752070f6cc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.1 acc 0.771\n",
      "Time taken: 12.653993299999911\n",
      "Fitted vocab size: 564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09078b509bc1490eb96729d045518b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.15 acc 0.724\n",
      "Time taken: 16.364532600000075\n",
      "Fitted vocab size: 541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff927bef8a134a909b73cc928cf74fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.2 acc 0.663\n",
      "Time taken: 19.118676999999934\n",
      "Fitted vocab size: 557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e71a599e75428590c36708fe4e6a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.25 acc 0.641\n",
      "Time taken: 21.702969700000267\n",
      "Finished size 10\n",
      "Fitted vocab size: 1214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a969cc253f4bef82c451e8978e8b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 50 prob_remove 0 acc 0.814\n",
      "Time taken: 88.51864780000005\n",
      "Fitted vocab size: 1205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec7dfe823fb4140a3d34b134502ec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 50 prob_remove 0.05 acc 0.826\n",
      "Time taken: 161.12651460000006\n",
      "Fitted vocab size: 1185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b7f4e4bbe749d09845a87ad1da5539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 50 prob_remove 0.1 acc 0.801\n",
      "Time taken: 230.84747490000018\n",
      "Fitted vocab size: 1172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e25ead659d4659bcabd329f38ce609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 50 prob_remove 0.15 acc 0.772\n",
      "Time taken: 297.65128920000006\n",
      "Fitted vocab size: 1169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6a369813a54cb1b87c8ffb285c9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 50 prob_remove 0.2 acc 0.75\n",
      "Time taken: 351.1806753999999\n",
      "Fitted vocab size: 1140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d528592608b4726bd10be1189517b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 50 prob_remove 0.25 acc 0.754\n",
      "Time taken: 387.79562609999994\n",
      "Finished size 50\n",
      "Fitted vocab size: 1369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3263b7e660da4aa4993166cdca607003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 100 prob_remove 0 acc 0.745\n",
      "Time taken: 10.423033499999747\n",
      "Fitted vocab size: 1352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dad10379e0487992a11fe79a1d9626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 100 prob_remove 0.05 acc 0.762\n",
      "Time taken: 19.41045099999974\n",
      "Fitted vocab size: 1345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf90bca62bc4045a2136e877fbb7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 100 prob_remove 0.1 acc 0.775\n",
      "Time taken: 26.91087749999997\n",
      "Fitted vocab size: 1328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae26d8d325649768a781dc9ae178a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 100 prob_remove 0.15 acc 0.753\n",
      "Time taken: 33.52473439999994\n",
      "Fitted vocab size: 1318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aed62688d8c476eadd4f2508dd7554e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 100 prob_remove 0.2 acc 0.741\n",
      "Time taken: 38.94242299999996\n",
      "Fitted vocab size: 1265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2276a684aa5347e9970957b4baf7d7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 100 prob_remove 0.25 acc 0.738\n",
      "Time taken: 43.592639399999825\n",
      "Finished size 100\n",
      "Fitted vocab size: 1381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4241085e0c3445e9939222dd98a540eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 500 prob_remove 0 acc 0.81\n",
      "Time taken: 129.43414510000002\n",
      "Fitted vocab size: 1385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e0904bd2df4b978d5dce76e0653539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 500 prob_remove 0.05 acc 0.821\n",
      "Time taken: 179.31988629999978\n",
      "Fitted vocab size: 1407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecf6ddd5ac24743b5c44c609964e134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 500 prob_remove 0.1 acc 0.818\n",
      "Time taken: 218.15307209999992\n",
      "Fitted vocab size: 1514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db949ad4d4304e8fb830faeecf0cf89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 500 prob_remove 0.15 acc 0.803\n",
      "Time taken: 252.97415590000037\n",
      "Fitted vocab size: 1826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f4d6778714353b74cea63bdb53277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 500 prob_remove 0.2 acc 0.803\n",
      "Time taken: 286.62825239999984\n",
      "Fitted vocab size: 2244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca44b0ed6dbe48b0b00223850b99aa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 500 prob_remove 0.25 acc 0.81\n",
      "Time taken: 312.8849458000004\n",
      "Finished size 500\n",
      "Fitted vocab size: 1697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdef977976e450a891018050bcd25f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0 acc 0.79\n",
      "Time taken: 18.390212999999676\n",
      "Fitted vocab size: 1738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ee673949704d7da84b081e34a1d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0.05 acc 0.814\n",
      "Time taken: 25.936208899999656\n",
      "Fitted vocab size: 1824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4354a4abc7914f989bef433a160bb4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0.1 acc 0.83\n",
      "Time taken: 33.294170199999826\n",
      "Fitted vocab size: 1896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010c3c491a6b4ae780759e591c5a4098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0.15 acc 0.835\n",
      "Time taken: 39.2092646000001\n",
      "Fitted vocab size: 1954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00bc16abe374b05bcb5677d8c1dadcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0.2 acc 0.843\n",
      "Time taken: 44.86072389999936\n",
      "Fitted vocab size: 2199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17044eef17e14f0197a26f3cbed63ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0.25 acc 0.848\n",
      "Time taken: 49.549273199999334\n",
      "Finished size 2000\n",
      "total time elapsed: 815.5294308662415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB, LogisticRegressionPytorch,OnehotTransformer\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "X_dev, Y_dev = get_data(\"dev\",cleanText = True)\n",
    "probs_remove = [0,0.05,0.1,0.15,0.2,0.25]\n",
    "sizes = [10, 50, 100, 500, 2000]\n",
    "\n",
    "rows = len(probs_remove)*len(sizes)\n",
    "\n",
    "data = {\"percentage kept\":[0.0]*rows,\n",
    "       \"size\":[0]*rows,\n",
    "       \"score\":[0.0]*rows,\n",
    "       \"vocab size base\":[0]*rows,\n",
    "        \"vocab size final\":[0]*rows,\n",
    "       \"time taken\":[0.0]*rows}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "z = 0\n",
    "big_tic = time.time()\n",
    "for size in sizes:\n",
    "    tic = time.perf_counter()\n",
    "    X_gpt_all, Y_gpt_all = get_data(\"clean_gpt_\" + str(size))\n",
    "    X_gpt, Y_gpt = X_gpt_all[:-size], Y_gpt_all[:-size]\n",
    "    X_base, Y_base = X_gpt_all[-size:], Y_gpt_all[-size:]\n",
    "    model_base = LogisticRegression(max_iter=100)\n",
    "    model_base.fit(X_base, Y_base)\n",
    "    probs = [(probs[0], i) for i, probs in enumerate(model_base.predict_proba(X_gpt))]\n",
    "    probs_neg = sorted([j for j in probs if Y_gpt[j[1]] == 0 ],key=lambda tup: tup[0],reverse=True)\n",
    "    probs_pos = sorted([j for j in probs if Y_gpt[j[1]] == 1 ],key=lambda tup: tup[0])\n",
    "\n",
    "    for prob_remove in probs_remove:\n",
    "        df.at[z,\"size\"] = size\n",
    "        df.at[z,\"percentage kept\"] = (1-prob_remove)\n",
    "        \n",
    "        \n",
    "        df.at[z,\"vocab size base\"] = len(model_base[0].vocab)\n",
    "\n",
    "        keep_neg = int((1-prob_remove)*len(probs_neg))\n",
    "        keep_pos = int((1-prob_remove)*len(probs_pos))\n",
    "        \n",
    "        probs_neg = probs_neg[:keep_neg]\n",
    "        probs_pos = probs_pos[:keep_pos]\n",
    "        \n",
    "        final_probs = probs_neg+probs_pos\n",
    "        shuffle(final_probs)\n",
    "        gpt_indices = [i[1] for i in final_probs]\n",
    "        X_gpt_pruned = [X_gpt[i] for i in gpt_indices]\n",
    "        Y_gpt_pruned = [Y_gpt[i] for i in gpt_indices]\n",
    "        X_all = X_base + X_gpt_pruned\n",
    "        Y_all = Y_base + Y_gpt_pruned\n",
    "        \n",
    "        transformer = OnehotTransformer(ngram_range=(1, 1), min_df=0.001, max_df=0.5, verbose_vocab=True)\n",
    "        transformer.fit(X_all,Y_all)\n",
    "        X_all = transformer.transform(X_all)\n",
    "        \n",
    "        df.at[z,\"vocab size final\"] = len(X_all[0])\n",
    "        model = LogisticRegressionPytorch(input_dim=len(X_all[0]),epochs=30,progress_bar=True)\n",
    "        model.train(X_all,Y_all,batch_size=64)\n",
    "\n",
    "        acc = model.score(transformer.transform(X_dev),Y_dev)\n",
    "        print(\"size\",size,\"prob_remove\",prob_remove,\"acc\",acc)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"Time taken:\",toc-tic)\n",
    "        df.at[z,\"time taken\"] = toc-tic\n",
    "        df.at[z,\"score\"] = acc\n",
    "        z += 1\n",
    "    print(f\"Finished size {size}\")\n",
    "print(\"total time elapsed:\",time.time()-big_tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d17758-5553-4188-a798-dc239e8aea5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage kept</th>\n",
       "      <th>size</th>\n",
       "      <th>score</th>\n",
       "      <th>vocab size base</th>\n",
       "      <th>vocab size final</th>\n",
       "      <th>time taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0.776</td>\n",
       "      <td>180</td>\n",
       "      <td>544</td>\n",
       "      <td>4.357952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.95</td>\n",
       "      <td>10</td>\n",
       "      <td>0.786</td>\n",
       "      <td>180</td>\n",
       "      <td>541</td>\n",
       "      <td>8.680736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>10</td>\n",
       "      <td>0.771</td>\n",
       "      <td>180</td>\n",
       "      <td>541</td>\n",
       "      <td>12.653993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.85</td>\n",
       "      <td>10</td>\n",
       "      <td>0.724</td>\n",
       "      <td>180</td>\n",
       "      <td>564</td>\n",
       "      <td>16.364533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.80</td>\n",
       "      <td>10</td>\n",
       "      <td>0.663</td>\n",
       "      <td>180</td>\n",
       "      <td>541</td>\n",
       "      <td>19.118677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.75</td>\n",
       "      <td>10</td>\n",
       "      <td>0.641</td>\n",
       "      <td>180</td>\n",
       "      <td>557</td>\n",
       "      <td>21.702970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>50</td>\n",
       "      <td>0.814</td>\n",
       "      <td>943</td>\n",
       "      <td>1214</td>\n",
       "      <td>88.518648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.95</td>\n",
       "      <td>50</td>\n",
       "      <td>0.826</td>\n",
       "      <td>943</td>\n",
       "      <td>1205</td>\n",
       "      <td>161.126515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.90</td>\n",
       "      <td>50</td>\n",
       "      <td>0.801</td>\n",
       "      <td>943</td>\n",
       "      <td>1185</td>\n",
       "      <td>230.847475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.85</td>\n",
       "      <td>50</td>\n",
       "      <td>0.772</td>\n",
       "      <td>943</td>\n",
       "      <td>1172</td>\n",
       "      <td>297.651289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.80</td>\n",
       "      <td>50</td>\n",
       "      <td>0.750</td>\n",
       "      <td>943</td>\n",
       "      <td>1169</td>\n",
       "      <td>351.180675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.75</td>\n",
       "      <td>50</td>\n",
       "      <td>0.754</td>\n",
       "      <td>943</td>\n",
       "      <td>1140</td>\n",
       "      <td>387.795626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>100</td>\n",
       "      <td>0.745</td>\n",
       "      <td>1291</td>\n",
       "      <td>1369</td>\n",
       "      <td>10.423033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.95</td>\n",
       "      <td>100</td>\n",
       "      <td>0.762</td>\n",
       "      <td>1291</td>\n",
       "      <td>1352</td>\n",
       "      <td>19.410451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.90</td>\n",
       "      <td>100</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1291</td>\n",
       "      <td>1345</td>\n",
       "      <td>26.910877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.85</td>\n",
       "      <td>100</td>\n",
       "      <td>0.753</td>\n",
       "      <td>1291</td>\n",
       "      <td>1328</td>\n",
       "      <td>33.524734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>100</td>\n",
       "      <td>0.741</td>\n",
       "      <td>1291</td>\n",
       "      <td>1318</td>\n",
       "      <td>38.942423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1291</td>\n",
       "      <td>1265</td>\n",
       "      <td>43.592639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.00</td>\n",
       "      <td>500</td>\n",
       "      <td>0.810</td>\n",
       "      <td>3711</td>\n",
       "      <td>1381</td>\n",
       "      <td>129.434145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>500</td>\n",
       "      <td>0.821</td>\n",
       "      <td>3711</td>\n",
       "      <td>1385</td>\n",
       "      <td>179.319886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.90</td>\n",
       "      <td>500</td>\n",
       "      <td>0.818</td>\n",
       "      <td>3711</td>\n",
       "      <td>1407</td>\n",
       "      <td>218.153072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85</td>\n",
       "      <td>500</td>\n",
       "      <td>0.803</td>\n",
       "      <td>3711</td>\n",
       "      <td>1514</td>\n",
       "      <td>252.974156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.80</td>\n",
       "      <td>500</td>\n",
       "      <td>0.803</td>\n",
       "      <td>3711</td>\n",
       "      <td>1826</td>\n",
       "      <td>286.628252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>0.810</td>\n",
       "      <td>3711</td>\n",
       "      <td>2244</td>\n",
       "      <td>312.884946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.00</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.790</td>\n",
       "      <td>8966</td>\n",
       "      <td>1697</td>\n",
       "      <td>18.390213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.814</td>\n",
       "      <td>8966</td>\n",
       "      <td>1738</td>\n",
       "      <td>25.936209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.90</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.830</td>\n",
       "      <td>8966</td>\n",
       "      <td>1824</td>\n",
       "      <td>33.294170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.835</td>\n",
       "      <td>8966</td>\n",
       "      <td>1896</td>\n",
       "      <td>39.209265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.80</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.843</td>\n",
       "      <td>8966</td>\n",
       "      <td>1954</td>\n",
       "      <td>44.860724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.848</td>\n",
       "      <td>8966</td>\n",
       "      <td>2199</td>\n",
       "      <td>49.549273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percentage kept  size  score  vocab size base  vocab size final  \\\n",
       "0              1.00    10  0.776              180               544   \n",
       "1              0.95    10  0.786              180               541   \n",
       "2              0.90    10  0.771              180               541   \n",
       "3              0.85    10  0.724              180               564   \n",
       "4              0.80    10  0.663              180               541   \n",
       "5              0.75    10  0.641              180               557   \n",
       "6              1.00    50  0.814              943              1214   \n",
       "7              0.95    50  0.826              943              1205   \n",
       "8              0.90    50  0.801              943              1185   \n",
       "9              0.85    50  0.772              943              1172   \n",
       "10             0.80    50  0.750              943              1169   \n",
       "11             0.75    50  0.754              943              1140   \n",
       "12             1.00   100  0.745             1291              1369   \n",
       "13             0.95   100  0.762             1291              1352   \n",
       "14             0.90   100  0.775             1291              1345   \n",
       "15             0.85   100  0.753             1291              1328   \n",
       "16             0.80   100  0.741             1291              1318   \n",
       "17             0.75   100  0.738             1291              1265   \n",
       "18             1.00   500  0.810             3711              1381   \n",
       "19             0.95   500  0.821             3711              1385   \n",
       "20             0.90   500  0.818             3711              1407   \n",
       "21             0.85   500  0.803             3711              1514   \n",
       "22             0.80   500  0.803             3711              1826   \n",
       "23             0.75   500  0.810             3711              2244   \n",
       "24             1.00  2000  0.790             8966              1697   \n",
       "25             0.95  2000  0.814             8966              1738   \n",
       "26             0.90  2000  0.830             8966              1824   \n",
       "27             0.85  2000  0.835             8966              1896   \n",
       "28             0.80  2000  0.843             8966              1954   \n",
       "29             0.75  2000  0.848             8966              2199   \n",
       "\n",
       "    time taken  \n",
       "0     4.357952  \n",
       "1     8.680736  \n",
       "2    12.653993  \n",
       "3    16.364533  \n",
       "4    19.118677  \n",
       "5    21.702970  \n",
       "6    88.518648  \n",
       "7   161.126515  \n",
       "8   230.847475  \n",
       "9   297.651289  \n",
       "10  351.180675  \n",
       "11  387.795626  \n",
       "12   10.423033  \n",
       "13   19.410451  \n",
       "14   26.910877  \n",
       "15   33.524734  \n",
       "16   38.942423  \n",
       "17   43.592639  \n",
       "18  129.434145  \n",
       "19  179.319886  \n",
       "20  218.153072  \n",
       "21  252.974156  \n",
       "22  286.628252  \n",
       "23  312.884946  \n",
       "24   18.390213  \n",
       "25   25.936209  \n",
       "26   33.294170  \n",
       "27   39.209265  \n",
       "28   44.860724  \n",
       "29   49.549273  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d647a862-8585-4155-92d7-1a8a224f358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"classifier_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59169cf-5ace-43f3-a389-c3f6ea6c468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 1697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68ed3d3dd464503880cdfb98424ff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 0 acc 0.792\n",
      "Time taken: 15.895366999999624\n",
      "Fitted vocab size: 3885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfa341695d45309456f0c4254d4705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2000 prob_remove 1 acc 0.854\n",
      "Time taken: 11.450990199999978\n",
      "Finished size 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB, LogisticRegressionPytorch,OnehotTransformer\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "X_dev, Y_dev = get_data(\"dev\",cleanText = True)\n",
    "probs_remove = [0,1]\n",
    "sizes = [2000]\n",
    "\n",
    "rows = len(probs_remove)*len(sizes)\n",
    "\n",
    "data = {\"percentage kept\":[0.0]*rows,\n",
    "       \"size\":[0]*rows,\n",
    "       \"score\":[0.0]*rows,\n",
    "       \"vocab size base\":[0]*rows,\n",
    "        \"vocab size final\":[0]*rows,\n",
    "       \"time taken\":[0.0]*rows}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "z = 0\n",
    "for size in sizes:\n",
    "    \n",
    "    for prob_remove in probs_remove:\n",
    "        df.at[z,\"size\"] = size\n",
    "        df.at[z,\"percentage kept\"] = (1-prob_remove)\n",
    "        tic = time.perf_counter()\n",
    "        X_gpt_all, Y_gpt_all = get_data(\"clean_gpt_\" + str(size))\n",
    "        X_gpt, Y_gpt = X_gpt_all[:-size], Y_gpt_all[:-size]\n",
    "        X_base, Y_base = X_gpt_all[-size:], Y_gpt_all[-size:]\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        model.fit(X_base, Y_base)\n",
    "        df.at[z,\"vocab size base\"] = len(model[0].vocab)\n",
    "        \n",
    "        probs = [(probs[0], i) for i, probs in enumerate(model.predict_proba(X_gpt))]\n",
    "        \n",
    "        probs_neg = sorted([j for j in probs if Y_gpt[j[1]] == 0 ],key=lambda tup: tup[0],reverse=True)\n",
    "        probs_pos = sorted([j for j in probs if Y_gpt[j[1]] == 1 ],key=lambda tup: tup[0])\n",
    "\n",
    "        keep_neg = int((1-prob_remove)*len(probs_neg))\n",
    "        keep_pos = int((1-prob_remove)*len(probs_pos))\n",
    "        \n",
    "        probs_neg = probs_neg[:keep_neg]\n",
    "        probs_pos = probs_pos[:keep_pos]\n",
    "        \n",
    "#         pruned_probs = []\n",
    "#         for n_prob, idx in probs:\n",
    "#             if n_prob > 1-prob_remove:\n",
    "#                 if Y_gpt[idx] == 0:\n",
    "#                     pruned_probs.append((n_prob,idx))\n",
    "#             elif n_prob < prob_remove:\n",
    "#                 if Y_gpt[idx] == 1:\n",
    "#                     pruned_probs.append((n_prob, idx))\n",
    "#             else:\n",
    "#                 pruned_probs.append((n_prob, idx))\n",
    "#         sorted_probs = sorted(pruned_probs, key=lambda x:x[0])\n",
    "        final_probs = probs_neg+probs_pos\n",
    "        shuffle(final_probs)\n",
    "        gpt_indices = [i[1] for i in final_probs]\n",
    "        X_gpt_pruned = [X_gpt[i] for i in gpt_indices]\n",
    "        Y_gpt_pruned = [Y_gpt[i] for i in gpt_indices]\n",
    "        X_all = X_base + X_gpt_pruned\n",
    "        Y_all = Y_base + Y_gpt_pruned\n",
    "        \n",
    "        transformer = OnehotTransformer(ngram_range=(1, 1), min_df=0.001, max_df=0.5, verbose_vocab=True)\n",
    "        transformer.fit(X_all,Y_all)\n",
    "        X_all = transformer.transform(X_all)\n",
    "        \n",
    "        df.at[z,\"vocab size final\"] = len(X_all[0])\n",
    "        model = LogisticRegressionPytorch(input_dim=len(X_all[0]),epochs=30,progress_bar=True)\n",
    "        model.train(X_all,Y_all,batch_size=64)\n",
    "\n",
    "        acc = model.score(transformer.transform(X_dev),Y_dev)\n",
    "        print(\"size\",size,\"prob_remove\",prob_remove,\"acc\",acc)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"Time taken:\",toc-tic)\n",
    "        df.at[z,\"time taken\"] = toc-tic\n",
    "        df.at[z,\"score\"] = toc-tic\n",
    "        z += 1\n",
    "    print(f\"Finished size {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2797b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.5634847080630213\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.5405468025949953\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.5602409638554217\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.705746061167748\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.5389249304911955\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.6728452270620945\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.6847775718257646\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.5291936978683967\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.6742354031510658\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.7572984244670992\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.6101714550509731\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.732970342910102\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.8096617238183503\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.593141797961075\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.7884615384615384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('modules')\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for size in [10, 50, 100, 500, 2000]:\n",
    "    X_base, Y_base = get_data(\"n_\" + str(size), early_return=False)\n",
    "    X_dev, Y_dev = get_data(\"dev\")\n",
    "    LR = LogisticRegression(max_iter=100)\n",
    "    BNB = BernoulliNB()\n",
    "    CNB = ComplementNB()\n",
    "    models = [LR,BNB,CNB]\n",
    "    for model in models:\n",
    "        model.fit(X_base, Y_base)\n",
    "        acc = (model.predict(X_dev) == np.array(Y_dev)).mean()\n",
    "        print(model,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2533\n",
      "Fitted vocab size: 2533\n",
      "Fitted vocab size: 2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2209\n",
      "Fitted vocab size: 2237\n",
      "Fitted vocab size: 2390\n",
      "Fitted vocab size: 2339\n",
      "Fitted vocab size: 2410\n",
      "Fitted vocab size: 2209\n",
      "Fitted vocab size: 2237\n",
      "Fitted vocab size: 2390\n",
      "Fitted vocab size: 2339\n",
      "Fitted vocab size: 2410\n",
      "10 0.7515060240963856\n",
      "Fitted vocab size: 5721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5721\n",
      "Fitted vocab size: 5721\n",
      "Fitted vocab size: 5346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5346\n",
      "Fitted vocab size: 5387\n",
      "Fitted vocab size: 5305\n",
      "Fitted vocab size: 5211\n",
      "Fitted vocab size: 5034\n",
      "Fitted vocab size: 5346\n",
      "Fitted vocab size: 5387\n",
      "Fitted vocab size: 5305\n",
      "Fitted vocab size: 5211\n",
      "Fitted vocab size: 5034\n",
      "100 0.7643651529193698\n",
      "Fitted vocab size: 11599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('modules')\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import time\n",
    "\n",
    "for size in [10, 100, 500, 2000]:\n",
    "    X_gpt_all, Y_gpt_all = get_data(\"gpt_\" + str(size))\n",
    "    X_gpt, Y_gpt = X_gpt_all[:-size], Y_gpt_all[:-size]\n",
    "    X_base, Y_base = X_gpt_all[-size:], Y_gpt_all[-size:]\n",
    "    X_dev, Y_dev = get_data(\"dev\")\n",
    "    ps, scores = [], []\n",
    "    model = LogisticRegression(max_iter=100)\n",
    "    model.fit(X_base, Y_base)\n",
    "    probs = [(list(probs), i) for i, probs in enumerate(model.predict_proba(X_gpt))]\n",
    "    pruned_probs = []\n",
    "    for (n_prob, p_prob), idx in probs:\n",
    "        if n_prob > 0.8:\n",
    "            if Y_gpt[idx] == 0:\n",
    "                pruned_probs.append((n_prob,idx))\n",
    "        elif n_prob < 0.2:\n",
    "            if Y_gpt[idx] == 1:\n",
    "                pruned_probs.append((n_prob, idx))\n",
    "        else:\n",
    "            pruned_probs.append((n_prob, idx))\n",
    "    sorted_probs = sorted(pruned_probs, key=lambda x:x[0])\n",
    "    #final_probs = sorted_probs[:size*25]+sorted_probs[-size*25:]\n",
    "    final_probs = sorted_probs\n",
    "    gpt_indices = [i[1] for i in final_probs]\n",
    "    X_gpt_pruned = [X_gpt[i] for i in gpt_indices]\n",
    "    Y_gpt_pruned = [Y_gpt[i] for i in gpt_indices]\n",
    "    X_all = X_base + X_gpt_pruned\n",
    "    Y_all = Y_base + Y_gpt_pruned\n",
    "    estimators = [\n",
    "         ('lr', LogisticRegression(max_iter=100, ngram_range=(1, 1), min_df=1, max_df=1., verbose_vocab=True)),\n",
    "         ('bnb', BernoulliNB(ngram_range=(1, 1), min_df=1, max_df=1., verbose_vocab=True)),\n",
    "        ('cnb', ComplementNB(ngram_range=(1, 1), min_df=1, max_df=1., verbose_vocab=True))\n",
    "    ]\n",
    "    clf = StackingClassifier(\n",
    "         estimators=estimators, final_estimator=LR()\n",
    "    )\n",
    "\n",
    "    clf.fit(X_all, Y_all)\n",
    "    acc = (clf.predict(X_dev) == np.array(Y_dev)).mean()\n",
    "    print(size,acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
