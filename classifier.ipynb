{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b93c087-848a-46a2-8710-8bd2f88ea8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2ac010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 3885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db756acce564b7d95fd9a4bef175d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0 acc 0.853\n",
      "Time taken: 5.2468021\n",
      "Fitted vocab size: 3645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f370d94124c5412da094e3554aea76a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.05 acc 0.846\n",
      "Time taken: 3.6430374000000008\n",
      "Fitted vocab size: 3524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0270251a7ce43c1bf2029e6eb8aa517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.1 acc 0.835\n",
      "Time taken: 3.4877731999999995\n",
      "Fitted vocab size: 3337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387c2d7b013d4ab182839e2ee213819d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.15 acc 0.817\n",
      "Time taken: 2.9065099999999973\n",
      "Fitted vocab size: 3205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ebf006abeb4f25b2726ff0e53c9346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.2 acc 0.803\n",
      "Time taken: 2.7112513000000007\n",
      "Fitted vocab size: 3087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9d5e224ee44afbab96a2a61fc6cf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 10 prob_remove 0.25 acc 0.786\n",
      "Time taken: 2.6301860999999995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB, LogisticRegressionPytorch,OnehotTransformer\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "X_dev, Y_dev = get_data(\"dev\",cleanText = True)\n",
    "probs_remove = [0,0.05,0.1,0.15,0.2,0.25]\n",
    "sizes = [10, 50, 100, 500, 2000]\n",
    "\n",
    "rows = len(probs_remove)*len(sizes)\n",
    "\n",
    "data = {\"percentage kept\":[0]*rows,\n",
    "       \"size\":[0]*rows,\n",
    "       \"score\":[0.0]*rows,\n",
    "       \"vocab size base\":[0]*rows,\n",
    "        \"vocab size final\":[0]*rows,\n",
    "       \"time taken\":[0]*rows}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "z = 0\n",
    "for size in sizes:\n",
    "    \n",
    "    for prob_remove in probs_remove:\n",
    "        df.at[z,\"size\"] = size\n",
    "        df.at[z,\"percentage kept\"] = 1-prob_remove\n",
    "        tic = time.perf_counter()\n",
    "        X_gpt_all, Y_gpt_all = get_data(\"clean_gpt_\" + str(size))\n",
    "        X_gpt, Y_gpt = X_gpt_all[:-size], Y_gpt_all[:-size]\n",
    "        X_base, Y_base = X_gpt_all[-size:], Y_gpt_all[-size:]\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        model.fit(X_base, Y_base)\n",
    "        df.at[z,\"vocab size base\"] = len(model[0].vocab)\n",
    "        \n",
    "        probs = [(probs[0], i) for i, probs in enumerate(model.predict_proba(X_gpt))]\n",
    "        \n",
    "        probs_neg = sorted([j for j in probs if Y_gpt[j[1]] == 0 ],key=lambda tup: tup[0],reverse=True)\n",
    "        probs_pos = sorted([j for j in probs if Y_gpt[j[1]] == 1 ],key=lambda tup: tup[0])\n",
    "\n",
    "        keep_neg = int((1-prob_remove)*len(probs_neg))\n",
    "        keep_pos = int((1-prob_remove)*len(probs_pos))\n",
    "        \n",
    "        probs_neg = probs_neg[:keep_neg]\n",
    "        probs_pos = probs_pos[:keep_pos]\n",
    "        \n",
    "#         pruned_probs = []\n",
    "#         for n_prob, idx in probs:\n",
    "#             if n_prob > 1-prob_remove:\n",
    "#                 if Y_gpt[idx] == 0:\n",
    "#                     pruned_probs.append((n_prob,idx))\n",
    "#             elif n_prob < prob_remove:\n",
    "#                 if Y_gpt[idx] == 1:\n",
    "#                     pruned_probs.append((n_prob, idx))\n",
    "#             else:\n",
    "#                 pruned_probs.append((n_prob, idx))\n",
    "#         sorted_probs = sorted(pruned_probs, key=lambda x:x[0])\n",
    "        final_probs = probs_neg+probs_pos\n",
    "        shuffle(final_probs)\n",
    "        gpt_indices = [i[1] for i in final_probs]\n",
    "        X_gpt_pruned = [X_gpt[i] for i in gpt_indices]\n",
    "        Y_gpt_pruned = [Y_gpt[i] for i in gpt_indices]\n",
    "        X_all = X_base + X_gpt_pruned\n",
    "        Y_all = Y_base + Y_gpt_pruned\n",
    "        \n",
    "        transformer = OnehotTransformer(ngram_range=(1, 1), min_df=0.001, max_df=0.5, verbose_vocab=True)\n",
    "        transformer.fit(X_all,Y_all)\n",
    "        X_all = transformer.transform(X_all)\n",
    "        \n",
    "        df.at[z,\"vocab size final\"] = len(X_all[0])\n",
    "        model = LogisticRegressionPytorch(input_dim=len(X_all[0]),epochs=30,progress_bar=True)\n",
    "        model.train(X_all,Y_all,batch_size=64)\n",
    "\n",
    "        acc = model.score(transformer.transform(X_dev),Y_dev)\n",
    "        print(\"size\",size,\"prob_remove\",prob_remove,\"acc\",acc)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"Time taken:\",toc-tic)\n",
    "        df.at[z,\"time taken\"] = toc-tic\n",
    "        z += 1\n",
    "    print(f\"Finished size {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d647a862-8585-4155-92d7-1a8a224f358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"classifier_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2797b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.5634847080630213\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.5405468025949953\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.5602409638554217\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.705746061167748\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.5389249304911955\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.6728452270620945\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.6847775718257646\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.5291936978683967\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.6742354031510658\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.7572984244670992\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.6101714550509731\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.732970342910102\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', LogisticRegression())]) 0.8096617238183503\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', BernoulliNB())]) 0.593141797961075\n",
      "Pipeline(steps=[('onehot',\n",
      "                 OnehotTransformer(max_df=1.0, min_df=1, ngram_range=(1, 1),\n",
      "                                   verbose_vocab=False)),\n",
      "                ('clf', ComplementNB())]) 0.7884615384615384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('modules')\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for size in [10, 50, 100, 500, 2000]:\n",
    "    X_base, Y_base = get_data(\"n_\" + str(size), early_return=False)\n",
    "    X_dev, Y_dev = get_data(\"dev\")\n",
    "    LR = LogisticRegression(max_iter=100)\n",
    "    BNB = BernoulliNB()\n",
    "    CNB = ComplementNB()\n",
    "    models = [LR,BNB,CNB]\n",
    "    for model in models:\n",
    "        model.fit(X_base, Y_base)\n",
    "        acc = (model.predict(X_dev) == np.array(Y_dev)).mean()\n",
    "        print(model,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2533\n",
      "Fitted vocab size: 2533\n",
      "Fitted vocab size: 2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 2209\n",
      "Fitted vocab size: 2237\n",
      "Fitted vocab size: 2390\n",
      "Fitted vocab size: 2339\n",
      "Fitted vocab size: 2410\n",
      "Fitted vocab size: 2209\n",
      "Fitted vocab size: 2237\n",
      "Fitted vocab size: 2390\n",
      "Fitted vocab size: 2339\n",
      "Fitted vocab size: 2410\n",
      "10 0.7515060240963856\n",
      "Fitted vocab size: 5721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5721\n",
      "Fitted vocab size: 5721\n",
      "Fitted vocab size: 5346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 5346\n",
      "Fitted vocab size: 5387\n",
      "Fitted vocab size: 5305\n",
      "Fitted vocab size: 5211\n",
      "Fitted vocab size: 5034\n",
      "Fitted vocab size: 5346\n",
      "Fitted vocab size: 5387\n",
      "Fitted vocab size: 5305\n",
      "Fitted vocab size: 5211\n",
      "Fitted vocab size: 5034\n",
      "100 0.7643651529193698\n",
      "Fitted vocab size: 11599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('modules')\n",
    "from model import LogisticRegression, BernoulliNB, ComplementNB\n",
    "from get_data import get_data\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import time\n",
    "\n",
    "for size in [10, 100, 500, 2000]:\n",
    "    X_gpt_all, Y_gpt_all = get_data(\"gpt_\" + str(size))\n",
    "    X_gpt, Y_gpt = X_gpt_all[:-size], Y_gpt_all[:-size]\n",
    "    X_base, Y_base = X_gpt_all[-size:], Y_gpt_all[-size:]\n",
    "    X_dev, Y_dev = get_data(\"dev\")\n",
    "    ps, scores = [], []\n",
    "    model = LogisticRegression(max_iter=100)\n",
    "    model.fit(X_base, Y_base)\n",
    "    probs = [(list(probs), i) for i, probs in enumerate(model.predict_proba(X_gpt))]\n",
    "    pruned_probs = []\n",
    "    for (n_prob, p_prob), idx in probs:\n",
    "        if n_prob > 0.8:\n",
    "            if Y_gpt[idx] == 0:\n",
    "                pruned_probs.append((n_prob,idx))\n",
    "        elif n_prob < 0.2:\n",
    "            if Y_gpt[idx] == 1:\n",
    "                pruned_probs.append((n_prob, idx))\n",
    "        else:\n",
    "            pruned_probs.append((n_prob, idx))\n",
    "    sorted_probs = sorted(pruned_probs, key=lambda x:x[0])\n",
    "    #final_probs = sorted_probs[:size*25]+sorted_probs[-size*25:]\n",
    "    final_probs = sorted_probs\n",
    "    gpt_indices = [i[1] for i in final_probs]\n",
    "    X_gpt_pruned = [X_gpt[i] for i in gpt_indices]\n",
    "    Y_gpt_pruned = [Y_gpt[i] for i in gpt_indices]\n",
    "    X_all = X_base + X_gpt_pruned\n",
    "    Y_all = Y_base + Y_gpt_pruned\n",
    "    estimators = [\n",
    "         ('lr', LogisticRegression(max_iter=100, ngram_range=(1, 1), min_df=1, max_df=1., verbose_vocab=True)),\n",
    "         ('bnb', BernoulliNB(ngram_range=(1, 1), min_df=1, max_df=1., verbose_vocab=True)),\n",
    "        ('cnb', ComplementNB(ngram_range=(1, 1), min_df=1, max_df=1., verbose_vocab=True))\n",
    "    ]\n",
    "    clf = StackingClassifier(\n",
    "         estimators=estimators, final_estimator=LR()\n",
    "    )\n",
    "\n",
    "    clf.fit(X_all, Y_all)\n",
    "    acc = (clf.predict(X_dev) == np.array(Y_dev)).mean()\n",
    "    print(size,acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
