{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('modules')\n",
    "from train import train, score\n",
    "from get_data import get_data\n",
    "from model import LangID, LogisticRegression, ComplementNB, BernoulliNB\n",
    "from get_gpt_reviews import get_gpt_reviews\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "Xt, Yt = get_data(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5979f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4427ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for size in [2000]:\n",
    "    X_all, Y_all = get_data(\"gpt_\" + str(size))\n",
    "    assert len(get_data(\"n_\" + str(size))) == size # Should be 2000 in the end\n",
    "    new_len = len(X_all) - size\n",
    "    print(\"Augmented 'x' size of original:\", new_len/size, \"for size\", size)\n",
    "    ps, scores = [], []\n",
    "    for i in range(0, 101, 10):\n",
    "        p = i/100\n",
    "        data_size = int(size + p*new_len) # exclusive to avoid indexing [-1:]\n",
    "        X, Y = X_all[-data_size:], Y_all[-data_size:]\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        model.fit(X, Y)\n",
    "        acc = (model.predict(Xt) == np.array(Yt)).mean()\n",
    "        scores.append(acc)\n",
    "        ps.append(p)\n",
    "    plt.plot(ps, scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a8d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef521655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, scores = [], []\n",
    "for i in range(20, 101, 10):\n",
    "    p = i/100\n",
    "    data_size = int(base_len + p*new_len - 1) # exclusive to avoid indexing [-1:]\n",
    "    data = list(zip(X_all[-data_size:], Y_all[-data_size:])) # Go from back, so we always get base data\n",
    "    shuffle(data)\n",
    "    X, Y = zip(*data)\n",
    "    model, vocab = train(X, Y, epochs=15, embed_dim=100, lstm_dim=100, min_df=0., max_df=1., batch_size=64)\n",
    "    \n",
    "    scores.append(score(model, vocab, Xt, Yt))\n",
    "    ps.append(p)\n",
    "plt.plot(ps, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_df in [0]:\n",
    "    ps, scores = [], []\n",
    "    for i in range(0, 51, 10):\n",
    "        p = i/100\n",
    "        data_size = int(base_len + p*new_len - 1) # exclusive to avoid indexing [-1:]\n",
    "        X = X_all[-data_size:] # Go from back, so we always get base data\n",
    "        Y = Y_all[-data_size:]\n",
    "\n",
    "        model, vocab = train(X, Y, epochs=20, embed_dim=100, lstm_dim=100, min_df=3, batch_size=512)\n",
    "        scores.append(score(model, vocab, Xt, Yt))\n",
    "        ps.append(p)\n",
    "    plt.plot(ps, scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e96db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnehotTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def convert(self, sentence):# [[w1, w2, w3], [w1, w2, w3]]\n",
    "        output = [0]*len(self.vocab)\n",
    "        for word in sentence.split():\n",
    "            word = word.lower()\n",
    "            if word in self.vocab:\n",
    "                output[self.vocab[word]] = 1\n",
    "        return output\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        vectorizer = TfidfVectorizer(min_df = 25)\n",
    "        vectorizer.fit(X)\n",
    "        self.vocab = vectorizer.vocabulary_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = [self.convert(row) for row in X]\n",
    "        return X_\n",
    "\n",
    "ps, scores = [], []\n",
    "for i in range(0, 101, 5):\n",
    "    p = i/100\n",
    "    data_size = int(base_len + p*new_len - 1) # exclusive to avoid indexing [-1:]\n",
    "    X = X_all[-data_size:] # Go from back, so we always get base data\n",
    "    Y = Y_all[-data_size:]\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('onehot', OnehotTransformer()),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    pipe.fit(X, Y)\n",
    "    \n",
    "    scores.append(pipe.score(Xt, Yt))\n",
    "    ps.append(p)\n",
    "plt.plot(ps, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(\"gpt_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_data(\"gpt_\" + str(100))\n",
    "ps, scores = [], []\n",
    "model = LogisticRegression(max_iter=100)\n",
    "model.fit(X, Y)\n",
    "probs = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0.1 (of being positive)\n",
    "# l = 1 (positive)\n",
    "\n",
    "# p - l -> abs(0.01 - 1) > 0.5 \n",
    "# Will catch wrong predictions\n",
    "#idxs = sorted((p, l, i) for p, l, i in zip(probs[:,0], Y, range(len(probs))) if abs(p - l) < 0.5)\n",
    "idxs = sorted((p, i) for p, l, i in zip(probs[:,1], Y, range(len(probs))) if abs(p - l) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "X_all, Y_all = get_data(\"gpt_\" + str(size))\n",
    "for del_p in range(0, 51, 10):\n",
    "    del_size = int((del_p / 100)*len(idxs))//2\n",
    "    del_idxs = set(idxs[:del_size] + idxs[-del_size:])\n",
    "    print(len(del_idxs))\n",
    "    \n",
    "\n",
    "    X_all = np.array([x for i, x in enumerate(X_all) if i not in del_idxs])\n",
    "    Y_all = np.array([y for i, y in enumerate(Y_all) if i not in del_idxs])\n",
    "    \n",
    "    new_len = len(X_all) - size\n",
    "    ps, scores = [], []\n",
    "    for i in range(0, 101, 20):\n",
    "        p = i/100\n",
    "        data_size = int(size + p*new_len) # exclusive to avoid indexing [-1:]\n",
    "        X, Y = X_all[-data_size:], Y_all[-data_size:]\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        model.fit(X, Y)\n",
    "        acc = (model.predict(Xt) == np.array(Yt)).mean()\n",
    "        scores.append(acc)\n",
    "        ps.append(p)\n",
    "    plt.plot(ps, scores)\n",
    "    plt.title('Percentage of poor labels deleted: ' + str(del_p))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b15750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
