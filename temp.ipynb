{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb14f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used = cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('modules')\n",
    "from train import train, score\n",
    "from get_data import get_data\n",
    "from model import LangID, LogisticRegression, ComplementNB, BernoulliNB\n",
    "from get_gpt_reviews import get_gpt_reviews\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "Xt, Yt = get_data(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5979f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4427ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 'x' size of original: 199.0 for size 10\n"
     ]
    }
   ],
   "source": [
    "clean = True\n",
    "data_type = \"gpt\"\n",
    "ns = [10, 50, 100, 500, 2000]\n",
    "\n",
    "\n",
    "for n in ns:\n",
    "    data_type = 'clean_' if clean else '' + data_type + f\"_{n}\"\n",
    "    X_all, Y_all = get_data(data_type)\n",
    "    aug_len = len(X_all) - n\n",
    "    print(\"Augmented 'x' size of original:\", aug_len/n, \"for size\", n)\n",
    "    \n",
    "    ps, scores = [], []\n",
    "    for i in range(0, 101, 10):\n",
    "        p = i/100\n",
    "        data_size = int(n + p*aug_len) # exclusive to avoid indexing [-1:]\n",
    "        X, Y = X_all[-data_size:], Y_all[-data_size:]\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        model.fit(X, Y)\n",
    "        acc = (model.predict(Xt) == np.array(Yt)).mean()\n",
    "        scores.append(acc)\n",
    "        ps.append(p)\n",
    "    plt.plot(ps, scores)\n",
    "    plt.title('Size: ' + str(n))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef521655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, scores = [], []\n",
    "for i in range(20, 101, 10):\n",
    "    p = i/100\n",
    "    data_size = int(base_len + p*new_len - 1) # exclusive to avoid indexing [-1:]\n",
    "    data = list(zip(X_all[-data_size:], Y_all[-data_size:])) # Go from back, so we always get base data\n",
    "    shuffle(data)\n",
    "    X, Y = zip(*data)\n",
    "    model, vocab = train(X, Y, epochs=15, embed_dim=100, lstm_dim=100, min_df=0., max_df=1., batch_size=64)\n",
    "    \n",
    "    scores.append(score(model, vocab, Xt, Yt))\n",
    "    ps.append(p)\n",
    "plt.plot(ps, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_df in [0]:\n",
    "    ps, scores = [], []\n",
    "    for i in range(0, 51, 10):\n",
    "        p = i/100\n",
    "        data_size = int(base_len + p*new_len - 1) # exclusive to avoid indexing [-1:]\n",
    "        X = X_all[-data_size:] # Go from back, so we always get base data\n",
    "        Y = Y_all[-data_size:]\n",
    "\n",
    "        model, vocab = train(X, Y, epochs=20, embed_dim=100, lstm_dim=100, min_df=3, batch_size=512)\n",
    "        scores.append(score(model, vocab, Xt, Yt))\n",
    "        ps.append(p)\n",
    "    plt.plot(ps, scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e96db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnehotTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def convert(self, sentence):# [[w1, w2, w3], [w1, w2, w3]]\n",
    "        output = [0]*len(self.vocab)\n",
    "        for word in sentence.split():\n",
    "            word = word.lower()\n",
    "            if word in self.vocab:\n",
    "                output[self.vocab[word]] = 1\n",
    "        return output\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        vectorizer = TfidfVectorizer(min_df = 25)\n",
    "        vectorizer.fit(X)\n",
    "        self.vocab = vectorizer.vocabulary_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = [self.convert(row) for row in X]\n",
    "        return X_\n",
    "\n",
    "ps, scores = [], []\n",
    "for i in range(0, 101, 5):\n",
    "    p = i/100\n",
    "    data_size = int(base_len + p*new_len - 1) # exclusive to avoid indexing [-1:]\n",
    "    X = X_all[-data_size:] # Go from back, so we always get base data\n",
    "    Y = Y_all[-data_size:]\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('onehot', OnehotTransformer()),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    pipe.fit(X, Y)\n",
    "    \n",
    "    scores.append(pipe.score(Xt, Yt))\n",
    "    ps.append(p)\n",
    "plt.plot(ps, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f3edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9920405845147653, 4618), (0.9887748239701647, 10458), (0.9786618772489212, 9581), (0.9783390961017192, 6006), (0.9778834074586763, 6462)]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "X_all, Y_all = get_data(\"gpt_\" + str(n))\n",
    "ps, scores = [], []\n",
    "model = LogisticRegression(max_iter=100)\n",
    "model.fit(X_all, Y_all)\n",
    "probs = model.predict_proba(X_all)\n",
    "\n",
    "# Assume the following:\n",
    "# p = 0.1 (of being positive)\n",
    "# l = 1 (positive)\n",
    "\n",
    "# p - l -> abs(0.01 - 1) > 0.99 \n",
    "# Will catch wrong predictions\n",
    "poor_idxs = sorted([(abs(p - l), i) for p, l, i in zip(probs[:,1], Y_all, range(len(probs))) if abs(p - l) > 0.5], reverse=True)\n",
    "print(poor_idxs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for del_p in range(0, 51, 10):\n",
    "    del_size = int((del_p / 100)*len(poor_idxs))\n",
    "    del_idxs = set(poor_idxs[:del_size])\n",
    "    print('Deleting', len(del_idxs), 'constituting', len(del_idxs)/len(X_all), 'percent')\n",
    "    \n",
    "    X = [x for i, x in enumerate(X_all) if i not in del_idxs]\n",
    "    Y = [y for i, y in enumerate(Y_all) if i not in del_idxs]\n",
    "    \n",
    "    new_len = len(X) - size\n",
    "    ps, scores = [], []\n",
    "    for i in range(0, 101, 20):\n",
    "        p = i/100\n",
    "        data_size = int(size + p*new_len) # exclusive to avoid indexing [-1:]\n",
    "        x, y = X[-data_size:], Y[-data_size:]\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        model.fit(x, y)\n",
    "        acc = (model.predict(Xt) == np.array(Yt)).mean()\n",
    "        scores.append(acc)\n",
    "        ps.append(p)\n",
    "    plt.plot(ps, scores)\n",
    "    plt.title('Percentage of poor labels deleted: ' + str(del_p))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0286f999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5113d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 186\n",
      "0.5634847080630213\n",
      "Fitted vocab size: 180\n",
      "0.5619786839666358\n",
      "\n",
      "\n",
      "Fitted vocab size: 986\n",
      "0.705746061167748\n",
      "Fitted vocab size: 943\n",
      "0.7230074142724745\n",
      "\n",
      "\n",
      "Fitted vocab size: 1358\n",
      "0.6847775718257646\n",
      "Fitted vocab size: 1291\n",
      "0.7016913809082483\n",
      "\n",
      "\n",
      "Fitted vocab size: 3957\n",
      "0.7572984244670992\n",
      "Fitted vocab size: 3711\n",
      "0.7898517145505097\n",
      "\n",
      "\n",
      "Fitted vocab size: 9620\n",
      "0.8096617238183503\n",
      "Fitted vocab size: 8966\n",
      "0.8396663577386468\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df = 1\n",
    "max_df = 1.\n",
    "n_gram_range = (1, 1)\n",
    "for size in [10, 50, 100, 500, 2000]:\n",
    "    Xt, Yt = get_data(\"dev\")\n",
    "    X, Y = get_data(\"n_\" + str(size), early_return=False)\n",
    "    model = LogisticRegression(max_iter=100, ngram_range=n_gram_range, min_df=min_df, max_df=max_df, verbose_vocab=True)\n",
    "    model.fit(X, Y)\n",
    "    print((model.predict(Xt) == np.array(Yt)).mean())\n",
    "\n",
    "    Xt, Yt = get_data(\"dev\", cleanText=True)\n",
    "    X, Y = get_data(\"n_\" + str(size), early_return=False, cleanText=True)\n",
    "    model = LogisticRegression(max_iter=100, ngram_range=n_gram_range, min_df=min_df, max_df=max_df, verbose_vocab=True)\n",
    "    model.fit(X, Y)\n",
    "    print((model.predict(Xt) == np.array(Yt)).mean())\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f45ba32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 640\n",
      "0.7260194624652456\n",
      "Fitted vocab size: 590\n",
      "0.754633920296571\n",
      "\n",
      "\n",
      "Fitted vocab size: 1409\n",
      "0.696014828544949\n",
      "Fitted vocab size: 1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7634383688600556\n",
      "\n",
      "\n",
      "Fitted vocab size: 1546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7160565338276181\n",
      "Fitted vocab size: 1436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7311167747914736\n",
      "\n",
      "\n",
      "Fitted vocab size: 1884\n",
      "0.7524328081556997\n",
      "Fitted vocab size: 1806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7714318813716404\n",
      "\n",
      "\n",
      "Fitted vocab size: 2448\n",
      "0.7879981464318814\n",
      "Fitted vocab size: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821014828544949\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df = 5\n",
    "max_df = .5\n",
    "n_gram_range = (1, 1)\n",
    "for n in [10, 50, 100, 500, 2000]:\n",
    "    Xt, Yt = get_data(\"dev\")\n",
    "    X, Y = get_data(\"gpt_\" + str(n), early_return=False)\n",
    "    X, Y = X[-5000:], Y[-5000:]\n",
    "    model = LogisticRegression(max_iter=100, ngram_range=n_gram_range, min_df=min_df, max_df=max_df, verbose_vocab=True)\n",
    "    model.fit(X, Y)\n",
    "    print((model.predict(Xt) == np.array(Yt)).mean())\n",
    "\n",
    "    Xt, Yt = get_data(\"dev\", cleanText=True)\n",
    "    X, Y = get_data(\"gpt_\" + str(n), early_return=False, cleanText=True)\n",
    "    X, Y = X[-5000:], Y[-5000:]\n",
    "    model = LogisticRegression(max_iter=100, ngram_range=n_gram_range, min_df=min_df, max_df=max_df, verbose_vocab=True)\n",
    "    model.fit(X, Y)\n",
    "    print((model.predict(Xt) == np.array(Yt)).mean())\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98638885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted vocab size: 446\n",
      "0.7170991658943466\n",
      "Fitted vocab size: 406\n",
      "0.7652919369786839\n",
      "\n",
      "\n",
      "Fitted vocab size: 1089\n",
      "0.7048192771084337\n",
      "Fitted vocab size: 1039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7371408711770158\n",
      "\n",
      "\n",
      "Fitted vocab size: 1194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6989110287303059\n",
      "Fitted vocab size: 1154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7336654309545876\n",
      "\n",
      "\n",
      "Fitted vocab size: 1061\n",
      "0.7439759036144579\n",
      "Fitted vocab size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Christoffer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7849860982391103\n",
      "\n",
      "\n",
      "Fitted vocab size: 1384\n",
      "0.7859128822984245\n",
      "Fitted vocab size: 1311\n",
      "0.8093141797961075\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df = 10\n",
    "max_df = .5\n",
    "n_gram_range = (1, 1)\n",
    "for n in [10, 50, 100, 500, 2000]:\n",
    "    Xt, Yt = get_data(\"dev\")\n",
    "    X, Y = get_data(\"gpt_\" + str(n), early_return=False)\n",
    "    X, Y = X[-5000:], Y[-5000:]\n",
    "    model = LogisticRegression(max_iter=100, ngram_range=n_gram_range, min_df=min_df, max_df=max_df, verbose_vocab=True)\n",
    "    model.fit(X, Y)\n",
    "    print((model.predict(Xt) == np.array(Yt)).mean())\n",
    "\n",
    "    Xt, Yt = get_data(\"dev\", cleanText=True)\n",
    "    X, Y = get_data(\"gpt_\" + str(n), early_return=False, cleanText=True)\n",
    "    X, Y = X[-5000:], Y[-5000:]\n",
    "    model = LogisticRegression(max_iter=100, ngram_range=n_gram_range, min_df=min_df, max_df=max_df, verbose_vocab=True)\n",
    "    model.fit(X, Y)\n",
    "    print((model.predict(Xt) == np.array(Yt)).mean())\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
